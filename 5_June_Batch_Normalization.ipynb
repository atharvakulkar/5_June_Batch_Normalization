{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***Q1) Theory and Concepts***"
      ],
      "metadata": {
        "id": "CSCi8QjArDsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Explain the concept of batch Normalization in the context of ANN?"
      ],
      "metadata": {
        "id": "Ezf7a6hfrS7E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch Normalization is a technique used in ANN to improve the training process and performance. It involves normalising the inputs of each layer accross a mini batch during training ,stabilizing and accelerating the learning process.\n",
        "##Key Concepts\n",
        "1) Normalization: For each mini batch, the inputs are normalized by subtracting the batch mean and dividing the batch standard deviation.\n",
        "2) Scaling and Shifting: After Normazling , the normalized value is scaled and shifted using learnable parameters.\n",
        "##Benifits\n",
        "1) Improved Training Speed\n",
        "2) Higher Learning Rates\n",
        "3) Regularization effect\n",
        "4) Reduced Sensitivity to Initialization\n"
      ],
      "metadata": {
        "id": "IuaDFQTerx6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B4LOQ4mqsPHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Q2) Describe the benifits of using batch Normalization during training."
      ],
      "metadata": {
        "id": "Z4eefGZmtjVp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benefits:\n",
        "1) Improved Training Speed: Normalizing the inputs helps in stabilizing and accelerating the training process by reducing internal covariate shift.\n",
        "\n",
        "\n",
        "2) Higher Learning Rates: Batch normalization allows the use of higher learning rates, making the training faster.\n",
        "\n",
        "\n",
        "3) Regularization Effect: It introduces a slight regularization effect by adding noise due to the mini-batch statistics, which can reduce the need for other regularization methods like dropout.\n",
        "4) Reduced Sensitivity to Initialization: Helps reduce sensitivity to weight initialization, making the network more robust."
      ],
      "metadata": {
        "id": "nhCwzfckt164"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGYSiaI2t2yj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3) Discuss the working principle of batch normalization, including the normalization step and the learnable\n",
        "parameters."
      ],
      "metadata": {
        "id": "jLe8dwunuK2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Batch normalization (BatchNorm) is a technique used in artificial neural networks to improve training speed and stability. It normalizes the input to each layer in the network, making the training process more efficient and reliable. Here’s a detailed discussion of its working principle:\n",
        "\n",
        "### Working Principle of Batch Normalization:\n",
        "\n",
        "1. **Normalization Step**:\n",
        "   - For a given layer in the neural network, consider the input to this layer as a mini-batch of size \\( m \\), represented as \\( \\{ x_1, x_2, \\ldots, x_m \\} \\).\n",
        "   - **Compute Batch Mean**:\n",
        "     \\[\n",
        "     \\mu_B = \\frac{1}{m} \\sum_{i=1}^{m} x_i\n",
        "     \\]\n",
        "   - **Compute Batch Variance**:\n",
        "     \\[\n",
        "     \\sigma_B^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_i - \\mu_B)^2\n",
        "     \\]\n",
        "   - **Normalize the Inputs**: Subtract the batch mean and divide by the square root of the batch variance plus a small constant \\( \\epsilon \\) to avoid division by zero.\n",
        "     \\[\n",
        "     \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}\n",
        "     \\]\n",
        "\n",
        "2. **Scaling and Shifting**:\n",
        "   - After normalization, each normalized input \\( \\hat{x}_i \\) is scaled and shifted using two learnable parameters: \\(\\gamma\\) (scale) and \\(\\beta\\) (shift).\n",
        "     \\[\n",
        "     y_i = \\gamma \\hat{x}_i + \\beta\n",
        "     \\]\n",
        "   - \\(\\gamma\\) and \\(\\beta\\) are learned during the training process, allowing the model to undo the normalization if needed and maintain the representational capacity of the network.\n",
        "\n",
        "### Summary of Steps:\n",
        "1. **Input**: A mini-batch of inputs \\( \\{ x_1, x_2, \\ldots, x_m \\} \\).\n",
        "2. **Calculate Batch Statistics**:\n",
        "   - Batch mean \\( \\mu_B \\)\n",
        "   - Batch variance \\( \\sigma_B^2 \\)\n",
        "3. **Normalize**: Normalize the batch inputs \\( \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} \\)\n",
        "4. **Scale and Shift**: Apply the learned scale and shift parameters \\( y_i = \\gamma \\hat{x}_i + \\beta \\)\n",
        "\n",
        "### Learnable Parameters:\n",
        "- **\\(\\gamma\\) (Scale Parameter)**: Adjusts the normalized output’s variance. If \\(\\gamma\\) is greater than 1, it increases the variance, and if it is less than 1, it decreases the variance.\n",
        "- **\\(\\beta\\) (Shift Parameter)**: Adjusts the normalized output’s mean. It shifts the mean of the normalized outputs.\n",
        "\n",
        "### During Training and Inference:\n",
        "- **Training**: Batch statistics (mean and variance) are computed for each mini-batch and used for normalization.\n",
        "- **Inference**: Moving averages of the batch mean and variance (calculated during training) are used to normalize the inputs, ensuring consistency and stability.\n",
        "\n",
        "### Benefits:\n",
        "- **Accelerates Training**: Stabilizes the learning process by reducing internal covariate shift.\n",
        "- **Allows Higher Learning Rates**: Reduces the risk of the network getting stuck in local minima, enabling the use of higher learning rates.\n",
        "- **Regularization Effect**: Adds a slight regularization by introducing noise due to mini-batch statistics, which can reduce overfitting.\n",
        "\n",
        "In summary, batch normalization normalizes the inputs of each layer to have a mean of zero and a variance of one for each mini-batch, then scales and shifts them using learnable parameters. This improves the stability and speed of the training process, making the network more robust and efficient."
      ],
      "metadata": {
        "id": "FqERzu8NuqPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2)Implementation"
      ],
      "metadata": {
        "id": "rzEGBzMdu2wL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ast import AsyncFunctionDef\n",
        "## Importing req libraries\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "4O5LQ7rUu89M"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##loading the mnist data\n",
        "(X_train_full, y_train_full), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
        "X_trian_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "X_valid, X_train = X_trian_full[:5000], X_trian_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
      ],
      "metadata": {
        "id": "W12KLiflvt3y"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "LAYERS = [tf.keras.layers.Flatten(input_shape = [28,28]),\n",
        "          tf.keras.layers.Dense(300,kernel_initializer=\"he_normal\"),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "          tf.keras.layers.Dense(100,kernel_initializer=\"he_normal\"),\n",
        "          tf.keras.layers.LeakyReLU(),\n",
        "          tf.keras.layers.Dense(10,activation=\"softmax\")]\n",
        "\n",
        "model = tf.keras.models.Sequential(LAYERS)"
      ],
      "metadata": {
        "id": "FMrW8jm2wcvx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##now compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer = tf.keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics = [\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqMPGjQgyBMB",
        "outputId": "ee2ea830-0ef2-4401-d760-fc4c38b4c622"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kDa3OeoJy1cp",
        "outputId": "f3319b72-7ed1-4032-b526-e95f9f788db4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_4 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 300)               235500    \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 300)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 100)               30100     \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 100)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 266610 (1.02 MB)\n",
            "Trainable params: 266610 (1.02 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##calculating the trianing time\n",
        "start = time.time()\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid), verbose=2)\n",
        "\n",
        "#ending time\n",
        "end = time.time()\n",
        "\n",
        "# total time taken\n",
        "print(f\"Runtime of the program is {end - start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOVRvriNzMfF",
        "outputId": "40cf567c-c814-4c67-89de-6defac4eac2d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 - 8s - loss: 0.6819 - accuracy: 0.7693 - val_loss: 0.5044 - val_accuracy: 0.8286 - 8s/epoch - 5ms/step\n",
            "Epoch 2/10\n",
            "1719/1719 - 9s - loss: 0.4858 - accuracy: 0.8304 - val_loss: 0.4386 - val_accuracy: 0.8488 - 9s/epoch - 5ms/step\n",
            "Epoch 3/10\n",
            "1719/1719 - 8s - loss: 0.4443 - accuracy: 0.8436 - val_loss: 0.5537 - val_accuracy: 0.7934 - 8s/epoch - 5ms/step\n",
            "Epoch 4/10\n",
            "1719/1719 - 6s - loss: 0.4203 - accuracy: 0.8532 - val_loss: 0.4009 - val_accuracy: 0.8622 - 6s/epoch - 4ms/step\n",
            "Epoch 5/10\n",
            "1719/1719 - 8s - loss: 0.4038 - accuracy: 0.8587 - val_loss: 0.3859 - val_accuracy: 0.8678 - 8s/epoch - 5ms/step\n",
            "Epoch 6/10\n",
            "1719/1719 - 6s - loss: 0.3866 - accuracy: 0.8645 - val_loss: 0.3792 - val_accuracy: 0.8696 - 6s/epoch - 4ms/step\n",
            "Epoch 7/10\n",
            "1719/1719 - 7s - loss: 0.3754 - accuracy: 0.8684 - val_loss: 0.3765 - val_accuracy: 0.8718 - 7s/epoch - 4ms/step\n",
            "Epoch 8/10\n",
            "1719/1719 - 6s - loss: 0.3653 - accuracy: 0.8704 - val_loss: 0.3972 - val_accuracy: 0.8584 - 6s/epoch - 4ms/step\n",
            "Epoch 9/10\n",
            "1719/1719 - 7s - loss: 0.3558 - accuracy: 0.8747 - val_loss: 0.3644 - val_accuracy: 0.8682 - 7s/epoch - 4ms/step\n",
            "Epoch 10/10\n",
            "1719/1719 - 7s - loss: 0.3479 - accuracy: 0.8764 - val_loss: 0.3626 - val_accuracy: 0.8726 - 7s/epoch - 4ms/step\n",
            "Runtime of the program is 83.40382766723633\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Accuracy = 0.8764"
      ],
      "metadata": {
        "id": "NO3xqmWOztj3"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xD7WSVM20K1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##After Applying Batch Normalization"
      ],
      "metadata": {
        "id": "MOVDRfie0Nzy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del model\n",
        "\n"
      ],
      "metadata": {
        "id": "sD0Bt5GA0lz1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defing new model with batch normalization\n",
        "LAYERS_BN = [\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(300, activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "]\n",
        "\n",
        "model = tf.keras.models.Sequential(LAYERS_BN)"
      ],
      "metadata": {
        "id": "5bgKNvSZ0qyG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vhc06keR0xW9",
        "outputId": "50cc553c-786f-4c95-f125-1ac9846f9136"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_5 (Flatten)         (None, 784)               0         \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 784)               3136      \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 300)               235500    \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 300)               1200      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 100)               30100     \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 100)               400       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                1010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 271346 (1.04 MB)\n",
            "Trainable params: 268978 (1.03 MB)\n",
            "Non-trainable params: 2368 (9.25 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bn1 = model.layers[1]"
      ],
      "metadata": {
        "id": "NGAXs7ft001l"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for variable in bn1.variables:\n",
        "  print(variable.name, variable.trainable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf4gmg8K08jl",
        "outputId": "d6a5653b-50f0-4156-9429-9f08eef300fc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_normalization/gamma:0 True\n",
            "batch_normalization/beta:0 True\n",
            "batch_normalization/moving_mean:0 False\n",
            "batch_normalization/moving_variance:0 False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=tf.keras.optimizers.SGD(lr=1e-3),\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bI3cJ51J1CqV",
        "outputId": "600a0869-8bb6-4183-d7c4-699a1eb47693"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.SGD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now training & calculating the training time.\n",
        "\n",
        "# starting time\n",
        "start = time.time()\n",
        "\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10,\n",
        "                    validation_data=(X_valid, y_valid), verbose=2)\n",
        "\n",
        "#ending time\n",
        "end = time.time()\n",
        "\n",
        "# total time taken\n",
        "print(f\"Runtime of the program is {end - start}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Evw4WY3Z1Hy1",
        "outputId": "f623bd2a-bdcf-4ca8-b6e3-c1f3c883153c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1719/1719 - 12s - loss: 0.5292 - accuracy: 0.8148 - val_loss: 0.3837 - val_accuracy: 0.8668 - 12s/epoch - 7ms/step\n",
            "Epoch 2/10\n",
            "1719/1719 - 10s - loss: 0.3915 - accuracy: 0.8602 - val_loss: 0.3505 - val_accuracy: 0.8764 - 10s/epoch - 6ms/step\n",
            "Epoch 3/10\n",
            "1719/1719 - 10s - loss: 0.3567 - accuracy: 0.8716 - val_loss: 0.3493 - val_accuracy: 0.8736 - 10s/epoch - 6ms/step\n",
            "Epoch 4/10\n",
            "1719/1719 - 10s - loss: 0.3256 - accuracy: 0.8829 - val_loss: 0.3236 - val_accuracy: 0.8822 - 10s/epoch - 6ms/step\n",
            "Epoch 5/10\n",
            "1719/1719 - 11s - loss: 0.3040 - accuracy: 0.8900 - val_loss: 0.3130 - val_accuracy: 0.8866 - 11s/epoch - 6ms/step\n",
            "Epoch 6/10\n",
            "1719/1719 - 10s - loss: 0.2889 - accuracy: 0.8952 - val_loss: 0.3138 - val_accuracy: 0.8858 - 10s/epoch - 6ms/step\n",
            "Epoch 7/10\n",
            "1719/1719 - 10s - loss: 0.2757 - accuracy: 0.9004 - val_loss: 0.3129 - val_accuracy: 0.8852 - 10s/epoch - 6ms/step\n",
            "Epoch 8/10\n",
            "1719/1719 - 10s - loss: 0.2629 - accuracy: 0.9036 - val_loss: 0.3133 - val_accuracy: 0.8846 - 10s/epoch - 6ms/step\n",
            "Epoch 9/10\n",
            "1719/1719 - 11s - loss: 0.2496 - accuracy: 0.9095 - val_loss: 0.3057 - val_accuracy: 0.8900 - 11s/epoch - 6ms/step\n",
            "Epoch 10/10\n",
            "1719/1719 - 10s - loss: 0.2399 - accuracy: 0.9123 - val_loss: 0.3036 - val_accuracy: 0.8888 - 10s/epoch - 6ms/step\n",
            "Runtime of the program is 143.56583952903748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Accuracy 0.9123 :))"
      ],
      "metadata": {
        "id": "cjGN78ik1MCe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0TQs5FD0111G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q3) Discuss the advantages and potential limitations of batch normalization in improving the training of\n",
        "neural networks."
      ],
      "metadata": {
        "id": "O8cl3MoC17XJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Advantages of Batch Normalization:\n",
        "\n",
        "1. **Accelerates Training**:\n",
        "   - **Stabilizes Learning**: By normalizing the inputs of each layer, batch normalization reduces internal covariate shift, leading to more stable and faster convergence.\n",
        "   - **Higher Learning Rates**: Enables the use of higher learning rates, which can speed up training and lead to better model performance.\n",
        "\n",
        "2. **Improves Generalization**:\n",
        "   - **Regularization Effect**: The noise introduced by mini-batch statistics acts as a form of regularization, reducing overfitting and improving generalization to new data.\n",
        "\n",
        "3. **Reduces Sensitivity to Initialization**:\n",
        "   - **Robustness to Weights**: Batch normalization makes the network less sensitive to the initial values of the weights, making the training process more reliable.\n",
        "\n",
        "4. **Allows Deeper Networks**:\n",
        "   - **Training Deep Networks**: Facilitates the training of very deep networks by mitigating issues related to vanishing and exploding gradients.\n",
        "\n",
        "5. **Smoothes Loss Landscape**:\n",
        "   - **Easier Optimization**: By normalizing activations, batch normalization can create a smoother loss landscape, making optimization easier and reducing the likelihood of getting stuck in local minima.\n",
        "\n",
        "### Potential Limitations of Batch Normalization:\n",
        "\n",
        "1. **Additional Computation**:\n",
        "   - **Overhead**: Batch normalization introduces additional computation due to the calculation of mean and variance for each mini-batch, as well as the scaling and shifting operations.\n",
        "\n",
        "2. **Dependence on Batch Size**:\n",
        "   - **Small Batches**: The effectiveness of batch normalization can diminish with very small batch sizes, as the batch statistics may become noisy and less representative of the data distribution.\n",
        "   - **Memory Consumption**: Requires maintaining running averages of batch statistics during training, which can increase memory consumption.\n",
        "\n",
        "3. **Complexity**:\n",
        "   - **Implementation**: Adds complexity to the model and training process, especially when combining with other normalization techniques or specialized layers.\n",
        "\n",
        "4. **Inference Discrepancy**:\n",
        "   - **Training vs. Inference**: During inference, running averages of batch statistics are used, which might not be as accurate as the batch statistics used during training, potentially leading to a small discrepancy in performance.\n",
        "\n",
        "5. **Potential for Over-reliance**:\n",
        "   - **Regularization Dependency**: Relying too heavily on the regularization effect of batch normalization may lead to underutilization of other regularization techniques like dropout, which might be more suitable for certain architectures or tasks.\n",
        "\n",
        "### Summary:\n",
        "Batch normalization offers several advantages in improving the training of neural networks, including accelerated training, improved generalization, and enabling deeper networks. However, it also comes with potential limitations, such as additional computation, dependence on batch size, and implementation complexity. Despite these limitations, batch normalization remains a widely used and powerful technique in modern deep learning."
      ],
      "metadata": {
        "id": "WaXuWQP_2OUB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Jc_I4Uoz2PK8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}